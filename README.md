# ğŸ¤– Educational LLM Chatbot

A clean, modular ChatGPT-like interface built with Streamlit and OpenAI. Perfect for learning LLM integration, chat interfaces, and software architecture patterns.

## ğŸ¯ What You'll Learn

- **Modular Architecture**: Clean separation of concerns
- **Configuration Management**: API keys and settings handling
- **Provider Pattern**: Abstracted LLM service integration
- **Storage Interfaces**: Pluggable chat history implementations
- **Error Handling**: User-friendly error messages
- **Streamlit Integration**: Real-time UI with custom packages

## ğŸ—ï¸ Architecture

```
+---------------------+    +---------------------+    +---------------------+
|   Streamlit UI      |<-->|     chat_core/      |<-->|     OpenAI API      |
|                     |    |                     |    |                     |
| â€¢ Chat Input        |    | â€¢ config.py         |    | â€¢ GPT-4o-mini       |
| â€¢ Messages          |    | â€¢ provider.py       |    | â€¢ Streaming         |
| â€¢ Sidebar           |    | â€¢ history.py        |    | â€¢ Completions       |
|                     |    | â€¢ errors.py         |    |                     |
+---------------------+    +---------------------+    +---------------------+
```

## ğŸ“ Project Structure

```
sandbox-vanilla/
â”œâ”€â”€ app.py             # Main Streamlit app
â”œâ”€â”€ chat_core/         # Core chat package
â”‚   â”œâ”€â”€ config.py      # Configuration management
â”‚   â”œâ”€â”€ provider.py    # OpenAI integration
â”‚   â”œâ”€â”€ history.py     # Chat storage interfaces
â”‚   â”œâ”€â”€ errors.py      # Error handling
â”‚   â””â”€â”€ session.py     # Session management
â”œâ”€â”€ requirements.txt   # Dependencies
â”œâ”€â”€ compose.yaml       # Redis setup
â”œâ”€â”€ Makefile           # Development commands
â””â”€â”€ .env.sample        # Environment template
``````

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- OpenAI API key
- Docker (optional, for Redis)

### Setup
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Configure API key
cp .env.sample .env
# Edit .env with your OpenAI API key

# 3. Run the app
make dev  # Starts Redis + Streamlit
# OR
streamlit run app.py  # Streamlit only
```

Open `http://localhost:8501` in your browser.

## âš™ï¸ Configuration

### Environment Variables
```bash
OPENAI_API_KEY=your_key_here     # Required
OPENAI_TEMPERATURE=0.7           # AI creativity (0.0-2.0)
REDIS_URL=redis://localhost:6379/0  # Optional, for persistence
HISTORY_MAX_TURNS=20             # Max conversation length
```

### Temperature Control
- **Range**: 0.0 (deterministic) to 2.0 (very creative)
- **Default**: 0.7 (balanced)
- **Usage**: Set via environment variable or .env file
- **Logging**: Temperature logged at startup in dev mode

## ğŸ› ï¸ Development

### Makefile Commands
| Command | Description |
|---------|-------------|
| `make dev` | Start Redis + Streamlit |
| `make app` | Streamlit only |
| `make redis-up` | Start Redis |
| `make redis-down` | Stop Redis |
| `make lint` | Code linting |
| `make fmt` | Code formatting |

### Key Features
- **Always-on streaming** with stop button
- **Redis persistence** with graceful fallback
- **Export conversations** (Markdown/JSON)
- **Theme-aware UI** (light/dark)
- **Error handling** with user-friendly messages

## ğŸ“š Core Modules

### `chat_core/config.py`
Manages configuration with precedence: env vars > .env > Streamlit secrets > defaults.

```python
from chat_core.config import load_config
config = load_config()
```

### `chat_core/provider.py`
OpenAI integration with streaming support.

```python
from chat_core.provider import OpenAIProvider
provider = OpenAIProvider(config)
# Streaming
for chunk in provider.stream_complete(messages):
    print(chunk, end='')
```

### `chat_core/history.py`
Storage interfaces for chat persistence.

```python
from chat_core.history import StreamlitStore
store = StreamlitStore()
store.add_message("user", "Hello")
messages = store.get_messages()
```

### `chat_core/errors.py`
User-friendly error messages.

```python
from chat_core.errors import humanize_error
try:
    # API call
except Exception as e:
    error_msg = humanize_error(e)
```

## ğŸ”§ Troubleshooting

### Common Issues
- **API Key**: Ensure `OPENAI_API_KEY` is set correctly
- **Redis**: App falls back to in-memory storage if Redis unavailable
- **Header Clipping**: Try hard refresh if title appears cut off
- **Streaming**: Check internet connection and OpenAI status

### Debug Mode
Set `DEBUG_UI=1` to see UI diagnostics in the sidebar.

## ğŸš€ Future Enhancements

- FastAPI sidecar for REST API
- Multiple LLM provider support
- Advanced conversation features
- User authentication

## ğŸ“„ License

MIT License - feel free to use for learning and development.

---

**Happy Learning! ğŸ‰** Start with the basic setup and explore the modular architecture patterns.